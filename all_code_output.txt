==== ./config.json ====
{
    "NETWORK_DATA_PATH": "state/network.json",
    "NETWORKS_FOLDER_PATH": "state/network/",
    "PEER_FOLDER_PATH": "state/peer/"

}==== ./state/network.json ====
{
  "networks": [
    "baseline",
    "demo_net",
    "extreme_test",
    "load_test",
    "low_mem_big_obj",
    "ramp_test",
    "read_heavy",
    "write_burst_ttl"
  ]
}==== ./state/stats/extreme_test_20250607_041748.json ====
{
  "config": {
    "name": "extreme_test",
    "peers": 8,
    "memory_mb": 32,
    "value_size": 16384,
    "ghost_ratio": 0.15,
    "ttl_ratio": 0.25,
    "scenarios": [
      [
        2,
        400
      ],
      [
        4,
        800
      ],
      [
        8,
        200
      ],
      [
        16,
        100
      ]
    ],
    "seed": 42
  },
  "stages": [
    {
      "workers": 2,
      "reqs": 400,
      "ops": 1600,
      "dur": 2.1277727079868782,
      "thr": 751.9600162151658,
      "lat_avg": 2654.7394631578945,
      "lat_p95": 4174.791,
      "hits": 546,
      "misses": 79,
      "hit_rate": 0.8736,
      "evictions": 0,
      "bytes": 26265380
    },
    {
      "workers": 4,
      "reqs": 800,
      "ops": 6400,
      "dur": 6.115946957987035,
      "thr": 1046.4446542725507,
      "lat_avg": 3843.097635892116,
      "lat_p95": 6947.458,
      "hits": 2183,
      "misses": 401,
      "hit_rate": 0.8448142414860681,
      "evictions": 0,
      "bytes": 104799306
    },
    {
      "workers": 8,
      "reqs": 200,
      "ops": 3200,
      "dur": 2.9762375840218738,
      "thr": 1075.1829817550217,
      "lat_avg": 7296.828922122674,
      "lat_p95": 14509.0,
      "hits": 1125,
      "misses": 177,
      "hit_rate": 0.8640552995391705,
      "evictions": 0,
      "bytes": 131130129
    },
    {
      "workers": 16,
      "reqs": 100,
      "ops": 3200,
      "dur": 2.430047875008313,
      "thr": 1316.846483935652,
      "lat_avg": 11735.507655196385,
      "lat_p95": 18152.209,
      "hits": 1131,
      "misses": 146,
      "hit_rate": 0.8856695379796398,
      "evictions": 0,
      "bytes": 156147832
    }
  ]
}==== ./peercache/settings/settings.py ====
import json
from pydantic import BaseModel, Field


class Settings(BaseModel):
    NETWORK_DATA_PATH: str = Field(default=None)
    NETWORKS_FOLDER_PATH: str = Field(default=None)
    PEER_FOLDER_PATH: str = Field(default=None)

    @classmethod
    def from_json(cls, json_path: str) -> "Settings":
        with open(json_path, "r") as f:
            data = json.load(f)
        return cls(**data)


SETTINGS = Settings.from_json("config.json")
==== ./peercache/core/hashing.py ====
import hashlib
import bisect
from typing import List, Dict


def _h32(data: str) -> int:
    """Return a 32-bit *stable* hash for the given string."""
    return int(hashlib.md5(data.encode()).hexdigest()[:8], 16)


class ConsistentHashRing:
    """
    Consistent-hash ring with V virtual nodes per peer.
    """

    def __init__(self, peer_ids: List[str], virtual_nodes: int = 100) -> None:
        self.vnodes: Dict[int, str] = {}
        self.ring: List[int] = []

        for pid in set(peer_ids):
            for v in range(virtual_nodes):
                h = _h32(f"{pid}#{v}")
                self.vnodes[h] = pid
                self.ring.append(h)

        self.ring.sort()

    def get_n(self, key: str, n: int = 1) -> List[str]:
        """
        Return up to *n* distinct peer_ids responsible for *key*.

        If the requested replication factor exceeds the number of peers on the
        ring, the list is truncated to the available peers.
        """
        if not self.ring:
            return list()

        # Prevent infinite loop if n > unique peers
        max_distinct = len(set(self.vnodes.values()))
        n = max(1, min(n, max_distinct))

        h = _h32(key)
        idx = bisect.bisect(self.ring, h)

        result: List[str] = []
        while len(result) < n:
            pid = self.vnodes[self.ring[idx % len(self.ring)]]
            if pid not in result:
                result.append(pid)
            idx += 1

        return result
==== ./peercache/parser/registry.py ====
import json
from pathlib import Path
from peercache.settings.settings import SETTINGS

_REG_PATH = Path(SETTINGS.PEER_FOLDER_PATH) / "registry.json"


def _load() -> set[str]:
    if _REG_PATH.exists():
        return set(json.loads(_REG_PATH.read_text()))
    return set()


def _save(peers: set[str]):
    _REG_PATH.parent.mkdir(parents=True, exist_ok=True)
    _REG_PATH.write_text(json.dumps(sorted(peers), indent=2))


def add(peer_id: str):
    peers = _load()
    peers.add(peer_id)
    _save(peers)


def remove(peer_id: str):
    peers = _load()
    peers.discard(peer_id)
    _save(peers)


def list_peers() -> list[str]:
    return sorted(_load())
==== ./peercache/parser/peer.py ====
import json
import socket
import subprocess
import time
from pathlib import Path
from typing import Dict, Any, Optional
import threading
from functools import lru_cache
from pymemcache.client.base import Client

from peercache.settings.settings import SETTINGS
from peercache.parser.registry import add as _reg_add, remove as _reg_rm


class Peer:
    """
    Thin wrapper around a memcached daemon plus a local JSON metadata file.
    """

    def __init__(self, peer_id: str, port: int | None = None):
        self.id = peer_id
        self.port = port if port is not None else self._find_free_port()
        self.path: Path = Path(SETTINGS.PEER_FOLDER_PATH) / f"{self.id}.json"
        self.pid: Optional[int] = None  # populated on start()
        self._load_or_init()

    @staticmethod
    def _find_free_port(min_port: int = 12000, max_port: int = 30000) -> int:
        """Grab an unused TCP port in the given range."""
        for port in range(min_port, max_port):
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                try:
                    s.bind(("localhost", port))
                    return port
                except OSError:
                    continue
        raise RuntimeError("No free port available in the safe range.")

    def _load_or_init(self) -> None:
        if self.path.exists():
            data = json.loads(self.path.read_text())
            self.port = data["port"]
            self.pid = data.get("pid")
        else:
            if self.port == 0:
                raise ValueError("Port must be supplied for new peer")
            self._persist()

    def _persist(self) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        payload = {"id": self.id, "port": self.port}
        if self.pid:
            payload["pid"] = self.pid
        self.path.write_text(json.dumps(payload, indent=2))

    def start(self, memory_mb: int = 64) -> str:
        """
        Launch memcached, register the peer once confirmed alive.
        """
        proc = subprocess.Popen(
            ["memcached", "-d", "-m", str(memory_mb), "-p", str(self.port)],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )

        deadline = time.time() + 5
        self.pid = proc.pid
        client = self._client()

        while time.time() < deadline:
            try:
                client.stats()
                self._persist()
                _reg_add(self.id)
                return f"Peer {self.id} running on :{self.port}"
            except Exception:
                time.sleep(0.1)

        proc.terminate()
        raise RuntimeError(f"Failed to start peer {self.id} on :{self.port}")

    def stop(self) -> str:
        """
        Terminate the daemon and unregister the peer.
        """
        _get_client.cache_clear()
        return f"Peer {self.id} stopped."

    # ------------------------------------------------------------------ #
    # Cache operations
    # ------------------------------------------------------------------ #
    def _client(self) -> Client:
        """
        Return a *thread-local* cached Client so we don‚Äôt open thousands of
        sockets.  One (thread, port) ‚Üí one TCP connection.
        """
        return _get_client(threading.get_ident(), self.port)

    def set(self, key: str, value: str) -> None:
        self._client().set(key, value)

    def get(self, key: str) -> Optional[bytes]:
        return self._client().get(key)

    def stats(self) -> Dict[str, Any]:
        raw = self._client().stats()
        return {
            (k.decode() if isinstance(k, bytes) else k): (
                v.decode() if isinstance(v, bytes) else v
            )
            for k, v in raw.items()
        }


@lru_cache(maxsize=None)
def _get_client(thread_id: int, port: int) -> Client:
    """Return a single pymemcache.Client per (thread, port) tuple."""
    return Client(
        ("localhost", port),
        connect_timeout=0.2,
        timeout=1.0,
        no_delay=True,
    )==== ./peercache/parser/network.py ====
import json
from pathlib import Path
from typing import List

from peercache.settings.settings import SETTINGS
from peercache.parser.peer import Peer
from peercache.core.hashing import ConsistentHashRing


class Network:
    """
    A single Memcached network with a consistent-hash ring and optional replication.
    Each network is stored in state/network/<name>.json
    """

    def __init__(self, name: str, replication: int = 1, vnodes: int = 100) -> None:
        self.name = name
        self.peers: List[str] = []
        self.write = True
        self.replication = replication
        self.vnodes = vnodes
        self.file_path = Path(SETTINGS.NETWORKS_FOLDER_PATH) / f"{self.name}.json"

        self._load_or_initialize()
        self._build_ring()

    def _load_or_initialize(self):
        if self.file_path.exists():
            try:
                with open(self.file_path, "r") as f:
                    data = json.load(f)
                self.peers = data.get("peers", [])
                self.write = data.get("write", True)
                self.replication = data.get("replication", self.replication)
                self.vnodes = data.get("vnodes", self.vnodes)
            except (json.JSONDecodeError, IOError):
                self.peers = []
        else:
            self._save()

    def _save(self):
        payload = {
            "name": self.name,
            "peers": self.peers,
            "write": self.write,
            "replication": self.replication,
            "vnodes": self.vnodes,
        }
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
        self.file_path.write_text(json.dumps(payload, indent=2))

    def _build_ring(self):
        self.ring = ConsistentHashRing(self.peers, virtual_nodes=self.vnodes)

    def add_peer(self, peer_id: str) -> str:
        if peer_id not in self.peers:
            self.peers.append(peer_id)
            self._save()
            self._build_ring()
            return f"Peer '{peer_id}' added to network '{self.name}'."
        return f"Peer '{peer_id}' already exists in network '{self.name}'."

    def remove_peer(self, peer_id: str) -> str:
        if peer_id in self.peers:
            self.peers.remove(peer_id)
            self._save()
            self._build_ring()
            return f"Peer '{peer_id}' removed from network '{self.name}'."
        return f"Peer '{peer_id}' not found in network '{self.name}'."

    def cache_set(self, key: str, value: str) -> str:
        if not self.peers:
            return "No peers available."
        targets = self.ring.get_n(key, self.replication)
        for pid in targets:
            Peer(pid).set(key, value)
        return f"SET {key} replicated to {targets}"

    def cache_get(self, key: str) -> str:
        if not self.peers:
            return "No peers available."
        for pid in self.ring.get_n(key, self.replication):
            val = Peer(pid).get(key)
            if val is not None:
                return val.decode()
        return "MISS"

    def stats(self) -> str:
        return (
            f"Network: {self.name}\n"
            f"Peers  : {', '.join(self.peers) if self.peers else 'None'}\n"
            f"Write  : {self.write}\n"
            f"Replicas: {self.replication} | VNodes: {self.vnodes}"
        )

    def __str__(self) -> str:
        return self.stats()
==== ./peercache/parser/manager.py ====
import json
from pathlib import Path
from peercache.parser.network import Network
from peercache.settings.settings import SETTINGS


class NetworkManager:
    """
    Manages a collection of named Memcached-like networks.
    Loads and saves network state from the path specified in Settings.
    """

    def __init__(self) -> None:
        """
        Initialize the NetworkManager, loading persisted networks if available.

        Args:
            settings (Settings): Configuration settings instance.
        """
        self.networks: set[Network] = set()
        self.write: bool = True
        self._load()

    def _load(self) -> None:
        """
        Load networks from the JSON file specified by settings.NETWORK_DATA_PATH.
        """
        data_path = Path(SETTINGS.NETWORK_DATA_PATH)
        if data_path.exists():
            try:
                data = json.loads(data_path.read_text())
                self.networks = {Network(name) for name in data.get("networks", [])}
            except Exception:
                self.networks = set()
        else:
            self.networks = set()

    def _save(self) -> None:
        """
        Save current networks to the JSON file specified by settings.NETWORK_DATA_PATH.
        """
        data_path = Path(SETTINGS.NETWORK_DATA_PATH)
        names = sorted(n.name for n in self.networks)
        data_path.write_text(json.dumps({"networks": names}, indent=2))

    def create_network(self, name: str) -> str:
        """
        Create a new network with the given name and persist changes.

        Args:
            name (str): Network name to create.

        Returns:
            str: Result message.
        """
        if name not in {n.name for n in self.networks}:
            self.networks.add(Network(name))
            self._save()
            return f"Network '{name}' created successfully."
        return f"Network '{name}' already exists."

    def delete_network(self, name: str) -> str:
        """
        Delete the network with the given name and persist changes.

        Args:
            name (str): Network name to delete.

        Returns:
            str: Result message.
        """
        for network in list(self.networks):
            if network.name == name:
                self.networks.remove(network)

                network_file = Path(SETTINGS.NETWORKS_FOLDER_PATH) / f"{name}.json"
                if network_file.exists():
                    network_file.unlink()

                self._save()
                return f"Network '{name}' deleted successfully."
        return f"Network '{name}' not found."

    def list_networks(self) -> str:
        """
        Return a formatted string listing all current network names.

        Returns:
            str: Networks summary.
        """
        names = sorted(n.name for n in self.networks)
        if names:
            return "Networks:\n" + "\n".join(f"  - {n}" for n in names)
        return "No networks available."

    def __str__(self) -> str:
        names = sorted(n.name for n in self.networks)
        return (
            "NetworkManager:\n"
            f"  Networks: {', '.join(names) if names else 'None'}\n"
            f"  Write   : {self.write}"
        )
==== ./testing/benchmark.py ====
import json
import os
import random
import signal
import statistics as stats
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Sequence, Tuple

import matplotlib.pyplot as plt

from peercache.parser.manager import NetworkManager
from peercache.parser.network import Network
from peercache.parser.peer import Peer

from pathlib import Path
from datetime import datetime

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
def _rand_val(size: int) -> str:
    return os.urandom(size).hex()


def _pct(series: List[float], q: float) -> float:
    if not series:
        return 0.0
    k = int(round(q / 100 * (len(series) - 1)))
    return sorted(series)[k]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ workload thread ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
def _one_worker(
    net: Network,
    wid: int,
    reqs: int,
    value_size: int,
    ghost_ratio: float,
    ttl_ratio: float,
) -> Dict[str, List[float]]:
    """
    Warm-up SETs every key, then mixed read/write:
      - 80 % reads, 20 % writes
      - ghost_ratio chance of reading an unknown key
      - ttl_ratio of writes get expire=2 s
    """
    lat: List[float] = []
    hits = misses = writes = 0

    keys = [f"{wid}:{i}" for i in range(reqs)]

    # warm-up
    for k in keys:
        t0 = time.perf_counter_ns()
        net.cache_set(k, _rand_val(value_size))
        lat.append((time.perf_counter_ns() - t0) / 1_000)

    time.sleep(0.05)

    # mixed phase
    for _ in range(reqs):
        if random.random() < 0.8:  # READ
            if random.random() < ghost_ratio:
                k = f"ghost:{random.randint(0, 1_000_000)}"
            else:
                k = random.choice(keys)
            t0 = time.perf_counter_ns()
            res = net.cache_get(k)
            lat.append((time.perf_counter_ns() - t0) / 1_000)
            if res != "MISS":
                hits += 1
            else:
                misses += 1
        else:  # WRITE
            k = random.choice(keys)
            ttl = 2 if random.random() < ttl_ratio else 0
            for pid in net.ring.get_n(k, 1):
                Peer(pid)._client().set(k, _rand_val(value_size), expire=ttl)
            writes += 1

    return {"lat": lat, "hits": [hits], "misses": [misses], "writes": [writes]}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ single stage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
def _run_stage(
    net: Network,
    workers: int,
    reqs: int,
    value_size: int,
    ghost_ratio: float,
    ttl_ratio: float,
) -> Dict:
    agg = defaultdict(list)
    start = time.perf_counter()

    with ThreadPoolExecutor(max_workers=workers) as pool:
        futs = [
            pool.submit(
                _one_worker, net, w, reqs, value_size, ghost_ratio, ttl_ratio
            )
            for w in range(workers)
        ]
        for f in as_completed(futs):
            part = f.result()
            for k, v in part.items():
                agg[k].extend(v)

    dur = time.perf_counter() - start
    total_ops = workers * reqs * 2
    thr = total_ops / dur

    hits = sum(agg["hits"])
    misses = sum(agg["misses"])
    hit_rate = hits / (hits + misses) if hits + misses else 0.0

    evictions = bytes_used = 0
    for pid in net.peers:
        s = Peer(pid).stats()
        evictions += int(s["evictions"])
        bytes_used += int(s["bytes"])

    return {
        "workers": workers,
        "reqs": reqs,
        "ops": total_ops,
        "dur": dur,
        "thr": thr,
        "lat_avg": stats.mean(agg["lat"]),
        "lat_p95": _pct(agg["lat"], 95),
        "hits": hits,
        "misses": misses,
        "hit_rate": hit_rate,
        "evictions": evictions,
        "bytes": bytes_used,
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ public API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
def run_benchmark(
    *,
    name: str,
    peers: int = 8,
    memory_mb: int = 32,
    value_size: int = 16_384,
    ghost_ratio: float = 0.15,
    ttl_ratio: float = 0.25,
    scenarios: Sequence[Tuple[int, int]] = ((2, 400), (4, 800)),
    seed: int = 42,
    plot: bool = True,
) -> List[Dict]:
    """
    Execute a full ramp test and (optionally) save 5 PNG charts.

    Returns the per-stage result list for programmatic inspection.
    """
    random.seed(seed)

    # --- spin up peers --------------------------------------------------- #
    mgr = NetworkManager()
    mgr.create_network(name)
    net = Network(name)
    peers_list: List[Peer] = []
    for i in range(peers):
        p = Peer(f"{name}_p{i}")
        p.start(memory_mb=memory_mb)
        peers_list.append(p)
        net.add_peer(p.id)

    # --- run workload matrix -------------------------------------------- #
    results = []
    for w, r in scenarios:
        print(f"\n‚ñ∂ Stage: {w} workers √ó {r} req")
        res = _run_stage(
            net, w, r, value_size, ghost_ratio, ttl_ratio
        )
        results.append(res)
        print(json.dumps(res, indent=2))
        time.sleep(2.5)  # give TTL items a chance to expire

    # --- visualisation --------------------------------------------------- #
    cfg = dict(
        name=name,
        peers=peers,
        memory_mb=memory_mb,
        value_size=value_size,
        ghost_ratio=ghost_ratio,
        ttl_ratio=ttl_ratio,
        scenarios=list(scenarios),
        seed=seed,
    )
    print(f"\nüîß Configuration:\n{json.dumps(cfg, indent=2)}")
    if plot:
        _make_plots(results, prefix=name)
    _save_json(cfg, results, prefix=name)

    # --- clean up -------------------------------------------------------- #
    for p in peers_list:
        p.stop()

    return results


def _save_json(config: Dict, stages: List[Dict], prefix: str) -> None:
    """
    Persist run configuration + per-stage results to
    state/stats/<prefix>_<timestamp>.json
    """
    out_dir = Path("results/simulations")
    out_dir.mkdir(parents=True, exist_ok=True)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = out_dir / f"{prefix}_{ts}.json"
    print(f"üíæ Saving results to ‚Üí {out_path}")

    payload = {"config": config, "stages": stages}
    out_path.write_text(json.dumps(payload, indent=2))
    print(f"üîñ Results saved ‚Üí {out_path}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ chart helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
def _make_plots(results: List[Dict], *, prefix: str = "") -> None:
    x = [r["workers"] for r in results]

    def _save(fig, fname):
        out_dir = Path("results/plots")
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / f"{prefix}_{fname}.png"
        fig.tight_layout()
        fig.savefig(out_path, dpi=120)
        print(f"üñºÔ∏è Saved plot ‚Üí {out_path}")

    # Throughput
    fig1 = plt.figure()
    plt.plot(x, [r["thr"] for r in results], "o-")
    plt.title("Throughput")
    plt.xlabel("Workers")
    plt.ylabel("Ops/s")
    plt.grid(True)
    _save(fig1, "throughput")

    # Latency
    fig2, ax = plt.subplots()
    ax.plot(x, [r["lat_avg"] for r in results], "o-", label="avg")
    ax.plot(x, [r["lat_p95"] for r in results], "s--", label="p95")
    ax.set_title("Latency")
    ax.set_xlabel("Workers")
    ax.set_ylabel("¬µs")
    ax.grid(True)
    ax.legend()
    _save(fig2, "latency")

    # Hit / miss %
    fig3 = plt.figure()
    hit_pct = [r["hit_rate"] * 100 for r in results]
    miss_pct = [100 - h for h in hit_pct]
    plt.bar(x, hit_pct, label="Hit %")
    plt.bar(x, miss_pct, bottom=hit_pct, label="Miss %", alpha=0.4)
    plt.title("Hit / Miss rate")
    plt.xlabel("Workers")
    plt.ylabel("%")
    plt.ylim(0, 105)
    plt.grid(axis="y")
    plt.legend()
    _save(fig3, "hit_miss")

    # Evictions
    fig4 = plt.figure()
    plt.plot(x, [r["evictions"] for r in results], "d-", color="tab:red")
    plt.title("Evictions")
    plt.xlabel("Workers")
    plt.ylabel("# evictions")
    plt.grid(True)
    _save(fig4, "evictions")

    fig5 = plt.figure()
    plt.plot(x, [r["bytes"] / 1_048_576 for r in results], "p-")
    plt.title("Bytes in use")
    plt.xlabel("Workers")
    plt.ylabel("MB")
    plt.grid(True)
    _save(fig5, "memory")

    # 1 ‚îÄ Latency CDF (per stage)
    fig6 = plt.figure()
    for r in results:
        sorted_lat = sorted(r["lat_dist"]) if "lat_dist" in r else []
        if not sorted_lat:
            continue
        y = [i / len(sorted_lat) * 100 for i in range(len(sorted_lat))]
        plt.plot(sorted_lat, y, label=f'{r["workers"]} workers')
    plt.title("Latency CDF")
    plt.xlabel("¬µs")
    plt.ylabel("Percentile")
    plt.grid(True)
    plt.legend()
    _save(fig6, "latency_cdf")

    # 3 ‚îÄ Throughput vs. Hit-Rate scatter
    fig8 = plt.figure()
    thr = [r["thr"] for r in results]
    hit = [r["hit_rate"] * 100 for r in results]
    plt.scatter(hit, thr)
    plt.title("Throughput vs. Hit Rate")
    plt.xlabel("Hit Rate (%)")
    plt.ylabel("Ops/s")
    plt.grid(True)
    for txt, x, y in zip([r["workers"] for r in results], hit, thr):
        plt.annotate(txt, (x, y))
    _save(fig8, "thr_vs_hit")


    plt.show()==== ./results/simulations/low_mem_big_obj_20250607_042625.json ====
{
  "config": {
    "name": "low_mem_big_obj",
    "peers": 5,
    "memory_mb": 4,
    "value_size": 64000,
    "ghost_ratio": 0.15,
    "ttl_ratio": 0.25,
    "scenarios": [
      [
        1,
        200
      ],
      [
        2,
        400
      ]
    ],
    "seed": 42
  },
  "stages": [
    {
      "workers": 1,
      "reqs": 200,
      "ops": 400,
      "dur": 0.2588467909954488,
      "thr": 1545.3156612902842,
      "lat_avg": 508.58851549295775,
      "lat_p95": 947.375,
      "hits": 101,
      "misses": 54,
      "hit_rate": 0.6516129032258065,
      "evictions": 52,
      "bytes": 19977926
    },
    {
      "workers": 2,
      "reqs": 400,
      "ops": 1600,
      "dur": 0.5024994580016937,
      "thr": 3184.0830363534583,
      "lat_avg": 538.3001631799164,
      "lat_p95": 969.959,
      "hits": 106,
      "misses": 528,
      "hit_rate": 0.167192429022082,
      "evictions": 939,
      "bytes": 20234079
    }
  ]
}==== ./results/simulations/write_burst_ttl_20250607_042834.json ====
{
  "config": {
    "name": "write_burst_ttl",
    "peers": 6,
    "memory_mb": 16,
    "value_size": 8192,
    "ghost_ratio": 0.05,
    "ttl_ratio": 0.8,
    "scenarios": [
      [
        8,
        800
      ],
      [
        16,
        1600
      ],
      [
        32,
        3200
      ]
    ],
    "seed": 42
  },
  "stages": [
    {
      "workers": 8,
      "reqs": 800,
      "ops": 12800,
      "dur": 5.964349541987758,
      "thr": 2146.0848177811695,
      "lat_avg": 3657.295493051937,
      "lat_p95": 5615.583,
      "hits": 4243,
      "misses": 871,
      "hit_rate": 0.8296832225263981,
      "evictions": 745,
      "bytes": 93374881
    },
    {
      "workers": 16,
      "reqs": 1600,
      "ops": 51200,
      "dur": 22.05734608299099,
      "thr": 2321.2221364872944,
      "lat_avg": 6859.067858274896,
      "lat_p95": 10705.417,
      "hits": 3530,
      "misses": 16931,
      "hit_rate": 0.1725233370802991,
      "evictions": 27723,
      "bytes": 93051582
    },
    {
      "workers": 32,
      "reqs": 3200,
      "ops": 204800,
      "dur": 91.93708162501571,
      "thr": 2227.610408989475,
      "lat_avg": 14351.000718073336,
      "lat_p95": 22966.666,
      "hits": 2431,
      "misses": 79529,
      "hit_rate": 0.029660810151293314,
      "evictions": 134185,
      "bytes": 94452126
    }
  ]
}==== ./results/simulations/extreme_test_20250607_042540.json ====
{
  "config": {
    "name": "extreme_test",
    "peers": 8,
    "memory_mb": 32,
    "value_size": 16384,
    "ghost_ratio": 0.15,
    "ttl_ratio": 0.25,
    "scenarios": [
      [
        2,
        400
      ],
      [
        4,
        800
      ],
      [
        8,
        200
      ],
      [
        16,
        100
      ]
    ],
    "seed": 42
  },
  "stages": [
    {
      "workers": 2,
      "reqs": 400,
      "ops": 1600,
      "dur": 1.486109208984999,
      "thr": 1076.6368920442849,
      "lat_avg": 1783.3000063157895,
      "lat_p95": 2815.541,
      "hits": 546,
      "misses": 79,
      "hit_rate": 0.8736,
      "evictions": 0,
      "bytes": 26265380
    },
    {
      "workers": 4,
      "reqs": 800,
      "ops": 6400,
      "dur": 4.876735041994834,
      "thr": 1312.3534382917946,
      "lat_avg": 3012.2952461964037,
      "lat_p95": 3945.875,
      "hits": 2187,
      "misses": 397,
      "hit_rate": 0.8463622291021672,
      "evictions": 0,
      "bytes": 104897800
    },
    {
      "workers": 8,
      "reqs": 200,
      "ops": 3200,
      "dur": 2.780022958002519,
      "thr": 1151.0696308419122,
      "lat_avg": 6845.4854903514815,
      "lat_p95": 11324.333,
      "hits": 1126,
      "misses": 176,
      "hit_rate": 0.8648233486943164,
      "evictions": 0,
      "bytes": 131162960
    },
    {
      "workers": 16,
      "reqs": 100,
      "ops": 3200,
      "dur": 2.4966304169793148,
      "thr": 1281.7275549625385,
      "lat_avg": 11995.708238095238,
      "lat_p95": 17750.0,
      "hits": 1131,
      "misses": 146,
      "hit_rate": 0.8856695379796398,
      "evictions": 0,
      "bytes": 156147832
    }
  ]
}==== ./results/simulations/baseline_20250607_033551.json ====
{
  "config": {
    "name": "baseline",
    "peers": 8,
    "memory_mb": 32,
    "value_size": 16384,
    "ghost_ratio": 0.15,
    "ttl_ratio": 0.25,
    "scenarios": [
      [
        2,
        400
      ],
      [
        4,
        800
      ],
      [
        8,
        200
      ],
      [
        16,
        100
      ]
    ],
    "seed": 42
  },
  "stages": [
    {
      "workers": 2,
      "reqs": 400,
      "ops": 1600,
      "dur": 0.7684328749892302,
      "thr": 2082.1597462529494,
      "lat_avg": 892.5719557894737,
      "lat_p95": 1511.542,
      "hits": 0,
      "misses": 0,
      "hit_rate": 0.0,
      "evictions": 0,
      "bytes": 54763336
    },
    {
      "workers": 4,
      "reqs": 800,
      "ops": 6400,
      "dur": 3.397231583017856,
      "thr": 1883.8868777720184,
      "lat_avg": 2075.957355982019,
      "lat_p95": 4445.375,
      "hits": 0,
      "misses": 0,
      "hit_rate": 0.0,
      "evictions": 0,
      "bytes": 105061960
    },
    {
      "workers": 8,
      "reqs": 200,
      "ops": 3200,
      "dur": 1.9088437920145225,
      "thr": 1676.4074741929717,
      "lat_avg": 4595.081067195038,
      "lat_p95": 10728.834,
      "hits": 0,
      "misses": 0,
      "hit_rate": 0.0,
      "evictions": 0,
      "bytes": 131327120
    },
    {
      "workers": 16,
      "reqs": 100,
      "ops": 3200,
      "dur": 1.8642747499980032,
      "thr": 1716.485190824704,
      "lat_avg": 8911.306336809177,
      "lat_p95": 18308.083,
      "hits": 0,
      "misses": 0,
      "hit_rate": 0.0,
      "evictions": 0,
      "bytes": 157329784
    }
  ]
}==== ./results/simulations/read_heavy_20250607_042617.json ====
{
  "config": {
    "name": "read_heavy",
    "peers": 8,
    "memory_mb": 32,
    "value_size": 16384,
    "ghost_ratio": 0.0,
    "ttl_ratio": 0.0,
    "scenarios": [
      [
        4,
        1000
      ],
      [
        8,
        2000
      ]
    ],
    "seed": 42
  },
  "stages": [
    {
      "workers": 4,
      "reqs": 1000,
      "ops": 8000,
      "dur": 6.204775916994549,
      "thr": 1289.329398357228,
      "lat_avg": 3089.9429503526485,
      "lat_p95": 4407.208,
      "hits": 3231,
      "misses": 0,
      "hit_rate": 1.0,
      "evictions": 0,
      "bytes": 131327560
    },
    {
      "workers": 8,
      "reqs": 2000,
      "ops": 32000,
      "dur": 24.074744333018316,
      "thr": 1329.1937624489021,
      "lat_avg": 5983.064293983086,
      "lat_p95": 8352.042,
      "hits": 6251,
      "misses": 6601,
      "hit_rate": 0.4863834422657952,
      "evictions": 10339,
      "bytes": 252090150
    }
  ]
}==== ./main.py ====
import typer

from peercache.parser.peer import Peer
from peercache.parser.network import Network
from peercache.parser.manager import NetworkManager
from peercache.parser.registry import list_peers as registry_list


app = typer.Typer()
manager: NetworkManager = NetworkManager()


@app.command("manager")
def network_manager(
    show: bool = typer.Option(False, "--show", help="Show all current networks."),
    create: str = typer.Option(None, "--create", help="Create a new network."),
    delete: str = typer.Option(None, "--delete", help="Delete an existing network."),
):
    if show:
        typer.echo(manager.list_networks())
    elif create:
        typer.echo(manager.create_network(create))
    elif delete:
        typer.echo(manager.delete_network(delete))
    else:
        typer.echo(
            "Please provide one of the following options: --show, --create <name>, or --delete <name>."
        )


@app.command("network")
def network_command(
    name: str = typer.Argument(..., help="Name of the network."),
    show: bool = typer.Option(False, "--show", help="Show network stats."),
    add: str = typer.Option(None, "--add", help="Add a peer to the network."),
    remove: str = typer.Option(
        None, "--remove", help="Remove a peer from the network."
    ),
):
    """
    Operate on an individual network by name.
    """
    if name not in {n.name for n in manager.networks}:
        typer.echo(f"Network '{name}' not found.")
        raise typer.Exit(code=1)

    network = Network(name)

    if show:
        typer.echo(network.stats())
    elif add:
        typer.echo(network.add_peer(add))
    elif remove:
        typer.echo(network.remove_peer(remove))
    else:
        typer.echo("Use one of: --show, --add <peer>, or --remove <peer>.")


@app.command("peer")
def peer_command(
    start: str = typer.Option(None, "--start", help="Start a new peer with given ID."),
    stop: str = typer.Option(None, "--stop", help="Stop a peer with given ID."),
    status: bool = typer.Option(False, "--status", help="Show all active peers."),
):
    if start:
        peer = Peer(start)
        peer.start()
        typer.echo(f"Started peer '{start}'.")
    elif stop:
        peer = Peer(stop)
        peer.stop()
        typer.echo(f"Stopped peer '{stop}'.")
    elif status:
        peers = registry_list()
        if not peers:
            typer.echo("No active peers.")
        else:
            typer.echo("Active peers:\n" + "\n".join(f"  - {p}" for p in peers))
    else:
        typer.echo("Use one of: --start <id>, --stop <id>, or --status.")


if __name__ == "__main__":
    app()
